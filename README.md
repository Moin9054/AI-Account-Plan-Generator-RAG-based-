# Account Plan Generator (RAG-based AI Assistant)

An AI-powered **Account Plan Generator** built using a **Retrieval-Augmented Generation (RAG)** architecture.
The system guides sales and strategy teams through structured account planning by grounding LLM responses in a curated knowledge base.

This project demonstrates a **production style RAG pipeline** with offline ingestion, vector search, and context-grounded generation.

---

## ğŸš€ What This Application Does

The application helps users **create structured account plans** by:
- Asking guided, domain specific questions
- Retrieving relevant knowledge from a predefined knowledge base
- Generating grounded, consistent, and reusable planning outputs

The AI does **not hallucinate** generic advice â€” all responses are **anchored to the knowledge base**.

---

## ğŸ§  Architecture Overview

This project is intentionally split into **two phases**:

### 1ï¸âƒ£ Offline Ingestion (One Time Process)
Used to prepare the knowledge base.

- Semantic chunking of domain documents
- Embedding generation using **VoyageAI**
- Vector storage in **Supabase PostgreSQL (pgvector)**

> These steps are executed once and never run during app runtime.

---

### 2ï¸âƒ£ Runtime Application (User Interaction)
Used during live usage.

- Streamlit-based conversational UI
- Vector similarity search against pgvector
- Context-grounded generation using **LLaMA 3.1 via OpenRouter**

```
User â†’ Streamlit UI
â†’ Vector Retrieval (Supabase pgvector)
â†’ Context Injection
â†’ LLaMA 3.1 (OpenRouter)
â†’ Structured Account Plan Output
```

---

## ğŸ› ï¸ Tech Stack

**Frontend**
- Streamlit

**LLM & Embeddings**
- LLaMA 3.1 (via OpenRouter)
- VoyageAI (embeddings)

**Vector Database**
- Supabase PostgreSQL
- pgvector extension

**Backend / Data**
- Python
- SQLAlchemy
- psycopg2

---

## ğŸ“ Project Structure

```
account-plan-generator/
â”œâ”€â”€ streamlit_UI.py          # Streamlit frontend
â”œâ”€â”€ Chat_pipeline.py         # RAG retrieval + LLM orchestration
â”œâ”€â”€ KB_embedding.py          # embedding ingestion
â”œâ”€â”€ semantic_chunking.py     # Knowledge base chunking
â”œâ”€â”€ Semantic_chunk.csv       # Preprocessed KB chunks
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“¤ Outputs Generated by the System

The application produces **structured, reusable business artifacts**, not just chat replies.

### ğŸ”¹ Primary Output: Account Plan Document

The AI generates a **multi-section account plan**, including:

1. **Account Overview**
   - Account name
   - Planning depth (Streamlined / Guided / Comprehensive)

2. **Strategic Importance**
   - Revenue tier classification
   - Strategic value assessment

3. **Account Complexity & Risk**
   - Stakeholder complexity
   - Competitive or relationship risks

4. **Growth Opportunities**
   - Expansion signals
   - Upsell / cross-sell indicators

5. **Recommended Planning Depth**
   - Rule-based guidance derived from KB logic

6. **Action-Oriented Planning Template**
   - Structured sections that sales teams can directly use

---

### ğŸ”¹ Exportable Formats

The generated account plan can be exported as:
- ğŸ“„ **DOCX**
- ğŸ“Š **PPT**
- ğŸ“• **PDF**

This makes the output usable by **non-technical business users**.

---

## ğŸ–Šï¸ Grounding Guarantee

- The system retrieves relevant KB chunks before every LLM call
- Retrieved context is injected into the prompt
- If information is **not present in the knowledge base**, the model does not fabricate answers

This ensures:
- Consistency
- Reduced hallucinations
- Auditability of AI outputs

---

## âš™ï¸ Running Locally

### 1ï¸âƒ£ Create Virtual Environment
```bash
python -m venv venv
venv\Scripts\activate
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Set Up Environment Variables
Create a `.env` file with:
```env
VOYAGE_API_KEY=your_voyage_api_key
OPENROUTER_API_KEY=your_openrouter_api_key
SUPABASE_DB_URL=your_supabase_connection_string
```

### 4ï¸âƒ£ Run Embedding Ingestion (One-Time)
```bash
python KB_embedding.py
```

### 5ï¸âƒ£ Launch the Application
```bash
streamlit run streamlit_UI.py
```

---

## ğŸ“Œ Notes

- The knowledge base must be ingested before the app can retrieve context.
- All LLM responses are grounded in retrieved chunks â€” no open ended generation.
- This project is designed for internal sales enablement and planning workflows.
